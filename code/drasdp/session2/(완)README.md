## 세션 2 과제 

# (세션3  예습) 읽기, 전처리
## 절대경로와 상대경로

* 절대경로: root(/)부터 파일의 위치까지의 전체적인 경로(URL)
    * 상대경로: 현재위치(.)을 기준으로 파일의 위치까지의 경로
        - '../'를 통해 상위 디렉토리로 올라감
***
## CSV
- 필드를 쉼표(,)로 구분하고 있는 텍스트 파일
- 메모장, 엑셀 등으로 열 수 있다.
- tsv(tab separated value)
***
## Excel
- 행과 열이 DataFrame과 일대일 대응함
- 불러올 시트를 특정하거나, 여러 sheet를 list를 통해 입력받을 수 있음
    - Ex. sheet_name=['Sheet 1', 'Sheet 2']
    - {'Sheet 1' : 데이터프레임1, 'Sheet 2' : 데이터프레임2}인 dictionary 타입으로 return 됨
    - .read_excel()
***
**index_col** : 불러온 데이터 중 하나의 열을 index로서 설정함.  
**usecols** : 불러온 데이터 중 특정 열만 선별하여 불러옴(메모리 관리).

`pd.read_csv('./~~~~.csv',index_col=0).head()   
pd.read_csv('data/SeoulFloating.csv',index_col='date').head()`  
*index_col은 정수 혹은 이름을 부여할 수 있다  
.head() 메소드는 DataFrame에서 첫번째 행부터 정된 개수(초기값 5)개의 행을 반환한다*

`pd.read_csv('./~~~~.csv',usecols=[0,1]).head()
pd.read_csv('data/SeoulFloating.csv',usecols=['date','hour']).head()`
*데이터에서 일부 열(커럼)만 선택해서 로드할 수 있다.
이는 .read_csv() 메소드 내 usecols 인자를 보면 알 수 있다.*
***
## 전처리
- 데이터의 누락, 중복 등의 오류를 수정하고 목적에 맞게 변형하는 과정
- 데이터 전처리
    - 데이터셋 확인 -> 결측값 처리 -> 이상값 처리 -> Feature Engineering
           - Feature Engineering : 도메인 지식을 통해 특징을 추출(PCA, Feature selection)
***
### 누락 데이터 확인(1)  
* info() 메소드 : 데이터프레임의 요약정보. non-NULL 즉 NaN이 아닌 유효한 값의 개수를 보여준다.
- value_counts(dropna=False) 메소드 : 각 열의 NaN 값의 개수를 알 수 있다.
- isnull() -> 누락 데이터이면 참. notnll() -> 유효데이터면 참.
- 누락데이터는 0 또는 '?' 값이 입력되기도 하는데 replace() 메소드를 통해 누락데이터를 통일한다.
***
### 결측값 처리(2)
1. 삭제
- : 결측값이 발생한 모든 관측치를 '전체삭제'할 수 있으며, 모델에 포함시킬 변수들 중 결측값이 발생한 모든 관측치를 '부분삭제'할 수 있다.
- '전체삭제'는 간편하지만 관측치가 줄어 모델의 유효성이 낮아질 수 있고, '부분삭제'는 모델에 따라 변수가 다르기 때문에 관리비용이 높아질 수 있다.
    - 분석대상이 갖는 '속성'과 관측값 즉 '레코드' 모두 삭제의 대상.
    - `df.dropna(axis=1, thresh=500)`
       -> NaN이 500개 이상인 '열'을 삭제하는 코드
- 삭제는 결측값이 무작위로 발생했을 때 사용한다.

2. 대체
- 데이터의 분포와 특성을 나타내는 평균값, 최빈값 등을 활용한다.
- fillna() 메소드
    - `a=df["열1"].mean(axis=0)` 
    - `df["열1"].fillna(a, inplace=True)`
    - a = '열1'에 대한 평균값. 열방햐으로 계산 'axis = 0'
    - df의 '열1'에서 결측치를 a 값으로 대체함. 원본 변경. 
- `df['열'].fillna(method = "ffill", inplace = True)`   
- `df['열'].fillna(method = "bfill", inplace = True)`   
    이웃한 값으로 대체
***
### 중복값 처리
- 행(row)은 대상이 가지는 모든 속성에 대한 관측값, 즉 레코드이다.
- 따라서 2개 이상 중복되는 레코드가 있으면 결과가 왜곡된다.
- `df["열1"].duplicated()`   -> 특정 열에 중복된 레코드가 있는지 확인
- `df["열2"].drop_duplicates()` -> 중복된 레코드를 제거
- `df.drop_duplicates(subset=["열4","열5"])`
    -> 열 4, 5를 기준으로 중복되느 행을 제거
***
### 이상치 처리
- 이상치(Outlier) : 기존 데이터들과 거리가 먼 데이터
1. 이상치 확인
    - df.describe(), BoxPlot을 통한 확인
    - Tukey Fences
        - 사분위 범위(IQR)를 기반으로, 두 가지 경우의 이상치를 판단함.
            - Q1 - (1.5 * IQR) 미만 : lower fence
            - Q3 + (1.5 * IQR) 초과 : upper fence
2. 이상치 처리
    1. 전체삭제 : Human error인 경우(오타, 비현실적응답 등)
    2. 다른 값으로 대체
        관측치의 절대량을 보존하기 위해 평균, 예측값 등으로 대체
    3. 변수화
        - 자연발생한 이상치의 경우 삭제나 대체로는 현상 설명이 어려움
        - 새로운 변수를 설정하는 등의 처리
    4. 리샘플링
        - 이상값을 분리하여 모델을 만듦.
        - 이상값을 포함하는 모델, 제외하는 모델을 모두 만들고 분리분석함
***
### 범주형 데이터 처리
- 연속데이터와 대비되는, 구간을 나누어 분석하는 데이터
    1. 구간분할 binning
    2. 더미변수
        - Binning과 반대로 범주형변수를 연속형변수로 변환하기 위해 사용함.
        - 카테고리를 나타내는 범주형데이터를 회귀분석 등 머신러닝 알고리즘에 사용할 수 있는 값으로 변환함.
        - 0, 1로 더미 변수를 사용함(특성이 있는지에 대한)
***
### 정규화
- 숫자 크기의 상대적 차이로 인한 왜곡을 방지하기 위해 각 열(속성)에 속하는 데이터값을 동일한 크기 기준으로 나누는 비율로 나타내는 정규화를 함.
- log 함수, 제곱근 취하기 등의 방법
1. StandardScaler
    - 평균을 0, 분산을 1로 갖는 스케일.
    - 정규분포를 따르는 가정에 적합
2. MinMaxScaler
    - 모든 feature가 0과 1 사이에 있게 함.(2차원에서도)
    - 데이터의 속성이 서로 다른 비율을 가질 때 같은 비율로 속성을 맞춤
    - 연산속도를 높이고 알고리즘을 최적화함
3. RobustScaler
    - StandadScaler와 마찬가지로 모든 특성들이 같은 크기를 갖게 하지만,
    - 평균과 분산 대신 median(중간값)과 quarile(사분위수)를 사용함.
    - 이상치에 영향받지 않음.

